{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f671019a",
   "metadata": {},
   "source": [
    "## 1 Data Preprocessing\n",
    "\n",
    "This notebook preprocesses the data, further 4 hours of training data and 4 hours of validated data are subsampled from the original dataset for the Sursilvan idiom. This subsample is going to be used to train the first prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6050883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa6eb8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"romansh-data\"\n",
    "FOLDER_NAMES = [folder for folder in os.listdir(DATA_ROOT) if not folder.endswith(\".tgz\")]\n",
    "IDIOM_FOLDER = \"rmsursilv-cc-2021-05-28\"\n",
    "TARGET_HOURS = {\n",
    "    \"train\": 4.0,\n",
    "    \"validated\": 4.0\n",
    "}\n",
    "RANDOM_SEED = 42\n",
    "OUTPUT_FOLDER = os.path.join(DATA_ROOT, \"sursilvan-small\")\n",
    "SPLITS = [\"train.tsv\", \"validated.tsv\", \"test.tsv\"]\n",
    "\n",
    "BASE_PATH = os.path.join(DATA_ROOT, IDIOM_FOLDER)\n",
    "CLIPS_PATH = os.path.join(BASE_PATH, \"clips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e807b75",
   "metadata": {},
   "source": [
    "As you can see from the examples, the sentences contain html tags that we need to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd66a1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1: <p>Hei tgau ansemen, igl mies nom √® Elin e gl'√® puspe eneda ouras per la vossa emissiun Minisguard. Ossa matte eneda avant tgi vusoters dastgessas eir glindesde per en'emda an la Svizra franzosa a scola? Scu fiss chegl per vusoters? Tot per franzos, novs conscolars e scolasts. Mattagn betg gist uscheia simpel all'antschatta. Ma exact chegl √≤ ena famiglia an la nossa seria igls Svizzers fatg chest'emda, numnadamaintg √® la famiglia Bernimoulin da Carouge sper Genevra sto per en'emda c√≤ tar nous an la rumantscheia, numnadamaintg a Sevgein.<br></p>...\n",
      "    2: <p>Schi vusoters levas saveir scu tgi Nicki √≤ passanto schiglio anc sia emda a Sevgein, alloura savez vurdar igls Cuntrasts digls 17 da november. Ed ossa nignsa tar en'otra famiglia, tar la famiglia digl Helveticus. Er chest'emda ans rachinta la famiglia digl Helveticus en'episoda or dall'istorgia dalla Svizra. E chest'eda vogl per ena battaglia agl Tessin e scu tg'igls svizzers √®n sa dustos, e chegl sainza armas.<br></p>...\n",
      "    3: <p>Hai, chegl √® alloura sto rafino da pattar crappa sen la suldada. Uscheia ensatge sumigliaint √≤gl er do c√≤ tar nous agl Grischun, angal tgi tar nous on betg igls omens cumbattia ancunter igl inimei, mabagn las donnas.<br></p>...\n"
     ]
    }
   ],
   "source": [
    "def print_example():\n",
    "  example_df = pd.read_csv(os.path.join(DATA_ROOT, FOLDER_NAMES[0], \"train.tsv\"), sep='\\t')\n",
    "  for i in range(min(3, len(example_df))):\n",
    "    print(f\"    {i+1}: {example_df['sentence'].iloc[i]}...\")\n",
    "\n",
    "print_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90215689",
   "metadata": {},
   "source": [
    "So we clean the html from the sentences in all files ending in .tsv from the `romansh-data` folder. Then we can verify that it worked with the cleaned examples from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4885ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1: Hei tgau ansemen, igl mies nom √® Elin e gl'√® puspe eneda ouras per la vossa emissiun Minisguard. Ossa matte eneda avant tgi vusoters dastgessas eir glindesde per en'emda an la Svizra franzosa a scola? Scu fiss chegl per vusoters? Tot per franzos, novs conscolars e scolasts. Mattagn betg gist uscheia simpel all'antschatta. Ma exact chegl √≤ ena famiglia an la nossa seria igls Svizzers fatg chest'emda, numnadamaintg √® la famiglia Bernimoulin da Carouge sper Genevra sto per en'emda c√≤ tar nous an la rumantscheia, numnadamaintg a Sevgein....\n",
      "    2: Schi vusoters levas saveir scu tgi Nicki √≤ passanto schiglio anc sia emda a Sevgein, alloura savez vurdar igls Cuntrasts digls 17 da november. Ed ossa nignsa tar en'otra famiglia, tar la famiglia digl Helveticus. Er chest'emda ans rachinta la famiglia digl Helveticus en'episoda or dall'istorgia dalla Svizra. E chest'eda vogl per ena battaglia agl Tessin e scu tg'igls svizzers √®n sa dustos, e chegl sainza armas....\n",
      "    3: Hai, chegl √® alloura sto rafino da pattar crappa sen la suldada. Uscheia ensatge sumigliaint √≤gl er do c√≤ tar nous agl Grischun, angal tgi tar nous on betg igls omens cumbattia ancunter igl inimei, mabagn las donnas....\n"
     ]
    }
   ],
   "source": [
    "def clean_html(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_file(tsv_path):\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    if 'sentence' not in df.columns:\n",
    "        print(f\"  Warning: No 'sentence' column found in {tsv_path}\")\n",
    "        return\n",
    "    df['sentence'] = df['sentence'].apply(clean_html)\n",
    "    df.to_csv(tsv_path, sep='\\t', index=False) \n",
    "    return len(df)\n",
    "\n",
    "tsv_file_paths = [os.path.join(DATA_ROOT, folder, f) for folder in FOLDER_NAMES for f in os.listdir(os.path.join(DATA_ROOT, folder)) if f.endswith('.tsv')]\n",
    "for path in tsv_file_paths:\n",
    "    preprocess_file(path)\n",
    "\n",
    "print_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a737ece",
   "metadata": {},
   "source": [
    "Then we subsample 4 hours each of training and validated data as well as the entire test set from the sursilvan idiom. This will be used in development to train leaner prototypes of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "514591b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subsampling train:  11%|‚ñà         | 754/6888 [00:00<00:01, 5569.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ train: 754 utterances, 4.00 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subsampling validated:  11%|‚ñà         | 750/6982 [00:00<00:00, 6436.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ validated: 750 utterances, 4.00 hours\n",
      "‚úÖ test set: 94 utterances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying clips: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [00:03<00:00, 463.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Mini Sursilvan folder ready at 'romansh-data/sursilvan-small'\n",
      "Contains:\n",
      " - 1504 audio files (referenced in TSVs)\n",
      " - train, validated, test TSVs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_audio_duration(path):\n",
    "  \"\"\"Return duration of a wav file in seconds.\"\"\"\n",
    "  try:\n",
    "    with sf.SoundFile(path) as f:\n",
    "      return len(f) / f.samplerate\n",
    "  except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not read {path}: {e}\")\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def subsample_split(df, split_name, target_hours):\n",
    "  \"\"\"Return a subsampled DataFrame totaling ~target_hours.\"\"\"\n",
    "  df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "  selected_rows = []\n",
    "  total_seconds = 0.0\n",
    "\n",
    "  for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Subsampling {split_name}\"):\n",
    "    audio_path = os.path.join(CLIPS_PATH, row[\"path\"])\n",
    "    duration = get_audio_duration(audio_path)\n",
    "    if duration == 0:\n",
    "      continue\n",
    "    if total_seconds + duration > target_hours * 3600:\n",
    "      break\n",
    "    selected_rows.append(row)\n",
    "    total_seconds += duration\n",
    "\n",
    "  sub_df = pd.DataFrame(selected_rows)\n",
    "  print(f\"‚úÖ {split_name}: {len(sub_df)} utterances, {total_seconds/3600:.2f} hours\")\n",
    "  return sub_df\n",
    "\n",
    "\n",
    "def copy_required_clips(df_list, output_clips_path):\n",
    "  \"\"\"Copy only audio files referenced in given list of DataFrames.\"\"\"\n",
    "  all_paths = set()\n",
    "  for df in df_list:\n",
    "    all_paths.update(df[\"path\"].tolist())\n",
    "\n",
    "  os.makedirs(output_clips_path, exist_ok=True)\n",
    "\n",
    "  for rel_path in tqdm(all_paths, desc=\"Copying clips\"):\n",
    "    src_path = os.path.join(CLIPS_PATH, rel_path)\n",
    "    dst_path = os.path.join(output_clips_path, rel_path)\n",
    "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "dfs_to_copy = []\n",
    "\n",
    "for split_name, hours in TARGET_HOURS.items():\n",
    "  tsv_path = os.path.join(BASE_PATH, f\"{split_name}.tsv\")\n",
    "  if not os.path.isfile(tsv_path):\n",
    "    print(f\"‚ùå Missing {split_name}.tsv\")\n",
    "    continue\n",
    "\n",
    "  df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "  sub_df = subsample_split(df, split_name, hours)\n",
    "  output_tsv = os.path.join(OUTPUT_FOLDER, f\"{split_name}.tsv\")\n",
    "  sub_df.to_csv(output_tsv, sep=\"\\t\", index=False)\n",
    "  dfs_to_copy.append(sub_df)\n",
    "\n",
    "test_tsv = os.path.join(BASE_PATH, \"test.tsv\")\n",
    "if os.path.isfile(test_tsv):\n",
    "  df_test = pd.read_csv(test_tsv, sep=\"\\t\")\n",
    "  output_test_tsv = os.path.join(OUTPUT_FOLDER, \"test.tsv\")\n",
    "  df_test.to_csv(output_test_tsv, sep=\"\\t\", index=False)\n",
    "  dfs_to_copy.append(df_test)\n",
    "  print(f\"‚úÖ test set: {len(df_test)} utterances\")\n",
    "\n",
    "output_clips_path = os.path.join(OUTPUT_FOLDER, \"clips\")\n",
    "copy_required_clips(dfs_to_copy, output_clips_path)\n",
    "\n",
    "print(f\"\\nüéâ Mini Sursilvan folder ready at '{OUTPUT_FOLDER}'\")\n",
    "print(\"Contains:\")\n",
    "print(f\" - {len(os.listdir(output_clips_path))} audio files (referenced in TSVs)\")\n",
    "print(\" - train, validated, test TSVs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TARS GPU)",
   "language": "python",
   "name": "tars_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
