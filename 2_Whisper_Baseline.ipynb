{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946601f6",
   "metadata": {},
   "source": [
    "## 2 Whisper Baseline\n",
    "\n",
    "Before fine-tuning our own model, we will first decode with the whisper model to get a baseline word error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b88cd",
   "metadata": {},
   "source": [
    "Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59f8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer, cer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import setproctitle\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROCESS_NAME = \"romansh-asr\"\n",
    "setproctitle.setproctitle(PROCESS_NAME)\n",
    "\n",
    "DATA_PATH = \"romansh-data/sursilvan-mini/\"\n",
    "WHISPER_MODEL = \"medium\"\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_FILE = \"whisper_baseline_results.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad505422",
   "metadata": {},
   "source": [
    "Checking GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb57815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Whisper Baseline for Romansh ASR\n",
      "============================================================\n",
      "Model: medium\n",
      "Device: cuda\n",
      "Data path: romansh-data/sursilvan-mini/\n",
      "============================================================\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "GPU Memory: 25.30 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Whisper Baseline for Romansh ASR\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {WHISPER_MODEL}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available - running on CPU (this will be slow!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f18a60",
   "metadata": {},
   "source": [
    "Defining some helpful helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ea4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    \"\"\"Remove HTML tags from text\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_data(tsv_path):\n",
    "    \"\"\"Load and prepare data from TSV file\"\"\"\n",
    "    print(f\"Loading data from {tsv_path}\")\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    print(\"Cleaning HTML from sentences...\")\n",
    "    df['clean_sentence'] = df['sentence'].apply(clean_html)\n",
    "    \n",
    "    print(\"\\nSample cleaning results:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"Original: {df['sentence'].iloc[i][:100]}...\")\n",
    "        print(f\"Cleaned:  {df['clean_sentence'].iloc[i][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_metrics(references, hypotheses):\n",
    "    \"\"\"Calculate WER and CER with better error handling\"\"\"\n",
    "    # Filter out empty or invalid pairs\n",
    "    valid_pairs = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # Skip if reference is empty or just whitespace\n",
    "        if not isinstance(ref, str) or not ref.strip():\n",
    "            continue\n",
    "        # Skip if hypothesis is empty (but still count these as errors if reference exists)\n",
    "        if not isinstance(hyp, str):\n",
    "            hyp = \"\"\n",
    "        \n",
    "        valid_pairs.append((ref.strip(), hyp.strip()))\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        print(\"Warning: No valid reference-hypothesis pairs found\")\n",
    "        return None, None\n",
    "    \n",
    "    refs, hyps = zip(*valid_pairs)\n",
    "    \n",
    "    try:\n",
    "        wer_score = wer(refs, hyps)\n",
    "        cer_score = cer(refs, hyps)\n",
    "        return wer_score, cer_score\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error calculating metrics: {e}\")\n",
    "        print(f\"First few references: {refs[:3]}\")\n",
    "        print(f\"First few hypotheses: {hyps[:3]}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fe6fa",
   "metadata": {},
   "source": [
    "Loading the whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0710b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Whisper medium model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading Whisper {WHISPER_MODEL} model...\")\n",
    "model = whisper.load_model(WHISPER_MODEL, device=DEVICE)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b9df57",
   "metadata": {},
   "source": [
    "Decoding with the whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec79818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_all_with_language(model, audio_paths):\n",
    "    \"\"\"Transcribe all audio files with a single progress bar\"\"\"\n",
    "    transcriptions = []\n",
    "    detected_languages = []\n",
    "    \n",
    "    # Single progress bar for all files\n",
    "    for audio_path in tqdm(audio_paths, desc=\"Transcribing audio files\"):\n",
    "        try:\n",
    "            if not os.path.exists(audio_path):\n",
    "                transcriptions.append(\"\")\n",
    "                detected_languages.append(\"missing\")\n",
    "                continue\n",
    "            \n",
    "            result = model.transcribe(\n",
    "                audio_path,\n",
    "                task=\"transcribe\",\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            transcriptions.append(result['text'].strip())\n",
    "            detected_languages.append(result.get('language', 'unknown'))\n",
    "            \n",
    "        except Exception as e:\n",
    "            transcriptions.append(\"\")\n",
    "            detected_languages.append(\"error\")\n",
    "    \n",
    "    return transcriptions, detected_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b13fc",
   "metadata": {},
   "source": [
    "Processing all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd41d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing train split\n",
      "==================================================\n",
      "Loading data from romansh-data/sursilvan-mini/train.tsv\n",
      "Cleaning HTML from sentences...\n",
      "\n",
      "Sample cleaning results:\n",
      "Original: <p><span b=\"2442.334\" e=\"2442.484\" s=\"100\" data-index=\"0\" class>Il</span> <span b=\"2442.484\" e=\"2442...\n",
      "Cleaned:  Il davos temps vegn bia discutau dalla rolla dalla dunna ella baselgia catolica, co vesis vus quella...\n",
      "\n",
      "Original: <p><span b=\"806.922\" e=\"807.322\" s=\"83\" data-index=\"0\" class>Suenter</span> <span b=\"807.352\" e=\"807...\n",
      "Cleaned:  Suenter ina prelecziun facultativa en dretg da bancas el studi ha quei plaschiu fetg bein a mi. Quei...\n",
      "\n",
      "Original: <p>E per veser quei stuein nus far in viadi da rodund 80 tochen varga 100 kilometers sut la tiara. <...\n",
      "Cleaned:  E per veser quei stuein nus far in viadi da rodund 80 tochen varga 100 kilometers sut la tiara. Schi...\n",
      "\n",
      "Total utterances: 92\n",
      "\n",
      "Running Whisper transcription...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing audio files:   0%|          | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Slovenian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:01<00:00, 504.86frames/s]\n",
      "Transcribing audio files:   1%|          | 1/92 [00:01<02:27,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:02<00:00, 890.17frames/s]\n",
      "Transcribing audio files:   2%|▏         | 2/92 [00:04<03:37,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Albanian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2031/2031 [00:02<00:00, 709.38frames/s]\n",
      "Transcribing audio files:   3%|▎         | 3/92 [00:07<04:05,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Italian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1350/1350 [00:01<00:00, 791.22frames/s]\n",
      "Transcribing audio files:   4%|▍         | 4/92 [00:09<03:36,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Lithuanian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3063/3063 [00:03<00:00, 768.31frames/s]\n",
      "Transcribing audio files:   5%|▌         | 5/92 [00:14<04:32,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2325/2325 [00:03<00:00, 639.13frames/s]\n",
      "Transcribing audio files:   7%|▋         | 6/92 [00:18<04:53,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [00:02<00:00, 474.98frames/s]\n",
      "Transcribing audio files:   8%|▊         | 7/92 [00:20<04:24,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Portuguese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2580/2580 [00:03<00:00, 836.56frames/s]\n",
      "Transcribing audio files:   9%|▊         | 8/92 [00:23<04:29,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [00:09<00:00, 203.20frames/s]\n",
      "Transcribing audio files:  10%|▉         | 9/92 [00:33<07:20,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1868/1868 [00:01<00:00, 1032.66frames/s]\n",
      "Transcribing audio files:  11%|█         | 10/92 [00:35<05:53,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Dutch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2541/2541 [00:03<00:00, 659.77frames/s]\n",
      "Transcribing audio files:  12%|█▏        | 11/92 [00:40<05:45,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Slovenian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1934/1934 [00:03<00:00, 518.33frames/s]\n",
      "Transcribing audio files:  13%|█▎        | 12/92 [00:44<05:35,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Occitan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2365/2365 [00:03<00:00, 633.40frames/s]\n",
      "Transcribing audio files:  14%|█▍        | 13/92 [00:48<05:26,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:01<00:00, 376.02frames/s]\n",
      "Transcribing audio files:  15%|█▌        | 14/92 [00:49<04:28,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Occitan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1964/1964 [00:01<00:00, 999.85frames/s] \n",
      "Transcribing audio files:  16%|█▋        | 15/92 [00:52<03:57,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 751/751 [00:01<00:00, 586.79frames/s]\n",
      "Transcribing audio files:  17%|█▋        | 16/92 [00:53<03:20,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Italian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1459/1459 [00:02<00:00, 685.05frames/s]\n",
      "Transcribing audio files:  18%|█▊        | 17/92 [00:56<03:12,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2645/2645 [00:03<00:00, 724.32frames/s]\n",
      "Transcribing audio files:  20%|█▉        | 18/92 [01:00<03:40,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Slovenian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1548/1548 [00:02<00:00, 584.78frames/s]\n",
      "Transcribing audio files:  21%|██        | 19/92 [01:03<03:36,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Latin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1665/1665 [00:02<00:00, 709.67frames/s]\n",
      "Transcribing audio files:  22%|██▏       | 20/92 [01:05<03:26,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Slovenian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1059/1059 [00:01<00:00, 608.13frames/s]\n",
      "Transcribing audio files:  23%|██▎       | 21/92 [01:07<03:05,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2151/2151 [00:14<00:00, 146.82frames/s]\n",
      "Transcribing audio files:  24%|██▍       | 22/92 [01:22<07:22,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Albanian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1025/1025 [00:01<00:00, 719.31frames/s]\n",
      "Transcribing audio files:  25%|██▌       | 23/92 [01:24<05:40,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2154/2154 [00:02<00:00, 992.61frames/s]\n",
      "Transcribing audio files:  26%|██▌       | 24/92 [01:26<04:44,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2246/2246 [00:10<00:00, 206.95frames/s]\n",
      "Transcribing audio files:  27%|██▋       | 25/92 [01:38<07:00,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Slovenian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [00:00<00:00, 620.61frames/s]\n",
      "Transcribing audio files:  28%|██▊       | 26/92 [01:38<05:05,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Portuguese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2781 [00:01<?, ?frames/s]\n",
      "Transcribing audio files:  28%|██▊       | 26/92 [01:40<04:14,  3.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal utterances: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning Whisper transcription...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m hypotheses, detected_langs = \u001b[43mtranscribe_all_with_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mdetected_language\u001b[39m\u001b[33m'\u001b[39m] = detected_langs\n\u001b[32m     36\u001b[39m references = df[\u001b[33m'\u001b[39m\u001b[33mclean_sentence\u001b[39m\u001b[33m'\u001b[39m].tolist()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtranscribe_all_with_language\u001b[39m\u001b[34m(model, audio_paths)\u001b[39m\n\u001b[32m     11\u001b[39m     detected_languages.append(\u001b[33m\"\u001b[39m\u001b[33mmissing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtranscribe\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m transcriptions.append(result[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].strip())\n\u001b[32m     22\u001b[39m detected_languages.append(result.get(\u001b[33m'\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/transcribe.py:295\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    293\u001b[39m     decode_options[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = all_tokens[prompt_reset_since:]\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m result: DecodingResult = \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m tokens = torch.tensor(result.tokens)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/transcribe.py:201\u001b[39m, in \u001b[36mtranscribe.<locals>.decode_with_fallback\u001b[39m\u001b[34m(segment)\u001b[39m\n\u001b[32m    198\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    200\u001b[39m options = DecodingOptions(**kwargs, temperature=t)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m decode_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m needs_fallback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    205\u001b[39m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m decode_result.compression_ratio > compression_ratio_threshold\n\u001b[32m    207\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/decoding.py:737\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    734\u001b[39m tokens = tokens.repeat_interleave(\u001b[38;5;28mself\u001b[39m.n_group, dim=\u001b[32m0\u001b[39m).to(audio_features.device)\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m tokens, sum_logprobs, no_speech_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[32m    740\u001b[39m audio_features = audio_features[:: \u001b[38;5;28mself\u001b[39m.n_group]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/decoding.py:687\u001b[39m, in \u001b[36mDecodingTask._main_loop\u001b[39m\u001b[34m(self, audio_features, tokens)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.sample_len):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m         logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    690\u001b[39m             i == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.no_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    691\u001b[39m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[32m    692\u001b[39m             probs_at_sot = logits[:, \u001b[38;5;28mself\u001b[39m.sot_index].float().softmax(dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/decoding.py:163\u001b[39m, in \u001b[36mPyTorchInference.logits\u001b[39m\u001b[34m(self, tokens, audio_features)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokens.shape[-\u001b[32m1\u001b[39m] > \u001b[38;5;28mself\u001b[39m.initial_token_length:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[32m    161\u001b[39m     tokens = tokens[:, -\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/model.py:242\u001b[39m, in \u001b[36mTextDecoder.forward\u001b[39m\u001b[34m(self, x, xa, kv_cache)\u001b[39m\n\u001b[32m    239\u001b[39m x = x.to(xa.dtype)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln(x)\n\u001b[32m    245\u001b[39m logits = (\n\u001b[32m    246\u001b[39m     x @ torch.transpose(\u001b[38;5;28mself\u001b[39m.token_embedding.weight.to(x.dtype), \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    247\u001b[39m ).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/model.py:169\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    167\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.attn(\u001b[38;5;28mself\u001b[39m.attn_ln(x), mask=mask, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.cross_attn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attn_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, xa, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n\u001b[32m    170\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.mlp_ln(x))\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/matuor/.venv/lib/python3.11/site-packages/whisper/model.py:41\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().forward(\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m).type(x.dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "splits = ['train', 'validated', 'test']\n",
    "all_results = {}\n",
    "language_summary = {}\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {split} split\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    tsv_path = os.path.join(DATA_PATH, f\"{split}.tsv\")\n",
    "    clips_path = os.path.join(DATA_PATH, \"clips\")\n",
    "    \n",
    "    if not os.path.exists(tsv_path):\n",
    "        print(f\"{split}.tsv not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    df = load_data(tsv_path)\n",
    "    \n",
    "    audio_paths = [os.path.join(clips_path, path) for path in df['path']]\n",
    "    \n",
    "    existing_indices = [i for i, path in enumerate(audio_paths) if os.path.exists(path)]\n",
    "    missing_count = len(audio_paths) - len(existing_indices)\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        print(f\"{missing_count} audio files missing, filtering them out\")\n",
    "        df = df.iloc[existing_indices].reset_index(drop=True)\n",
    "        audio_paths = [audio_paths[i] for i in existing_indices]\n",
    "    \n",
    "    print(f\"Total utterances: {len(df)}\")\n",
    "    \n",
    "    print(f\"\\nRunning Whisper transcription...\")\n",
    "    hypotheses, detected_langs = transcribe_all_with_language(model, audio_paths)\n",
    "    \n",
    "    df['detected_language'] = detected_langs\n",
    "    \n",
    "    references = df['clean_sentence'].tolist()\n",
    "    wer_score, cer_score = calculate_metrics(references, hypotheses)\n",
    "    \n",
    "    all_results[split] = {\n",
    "        'wer': wer_score,\n",
    "        'cer': cer_score,\n",
    "        'count': len(df),\n",
    "        'references': references[:5],\n",
    "        'hypotheses': hypotheses[:5],\n",
    "        'languages': detected_langs[:5]\n",
    "    }\n",
    "    \n",
    "    lang_counts = df['detected_language'].value_counts()\n",
    "    language_summary[split] = lang_counts\n",
    "    \n",
    "    print(f\"\\nResults for {split}:\")\n",
    "    print(f\"   Utterances: {len(df)}\")\n",
    "    if wer_score is not None:\n",
    "        print(f\"   WER: {wer_score:.4f} ({wer_score*100:.2f}%)\")\n",
    "        print(f\"   CER: {cer_score:.4f} ({cer_score*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"   No valid reference-hypothesis pairs\")\n",
    "    \n",
    "    print(f\"\\nLanguage detection distribution:\")\n",
    "    for lang, count in lang_counts.items():\n",
    "        print(f\"   {lang}: {count} ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a30681",
   "metadata": {},
   "source": [
    "Save results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSaving results to {OUTPUT_FILE}\")\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"Whisper Baseline Results\\n\")\n",
    "    f.write(f\"Model: {WHISPER_MODEL}\\n\")\n",
    "    f.write(f\"Date: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    for split, results in all_results.items():\n",
    "        f.write(f\"\\n{split.upper()} Split:\\n\")\n",
    "        f.write(f\"  Utterances: {results['count']}\\n\")\n",
    "        if results['wer'] is not None:\n",
    "            f.write(f\"  WER: {results['wer']:.4f}\\n\")\n",
    "            f.write(f\"  CER: {results['cer']:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n  Language Distribution:\\n\")\n",
    "        for lang, count in language_summary[split].items():\n",
    "            f.write(f\"    {lang}: {count}\\n\")\n",
    "        \n",
    "        f.write(\"\\n  Example transcriptions:\\n\")\n",
    "        for i, (ref, hyp, lang) in enumerate(zip(results['references'], \n",
    "                                                results['hypotheses'], \n",
    "                                                results['languages'])):\n",
    "            f.write(f\"    Ref {i+1}: {ref}\\n\")\n",
    "            f.write(f\"    Hyp {i+1}: {hyp}\\n\")\n",
    "            f.write(f\"    Lang {i+1}: {lang}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4ad360",
   "metadata": {},
   "source": [
    "Show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_utterances = 0\n",
    "for split in splits:\n",
    "    if split in all_results:\n",
    "        results = all_results[split]\n",
    "        print(f\"\\n{split.upper()}:\")\n",
    "        print(f\"  Utterances: {results['count']}\")\n",
    "        total_utterances += results['count']\n",
    "        if results['wer'] is not None:\n",
    "            print(f\"  WER: {results['wer']:.4f}\")\n",
    "            print(f\"  CER: {results['cer']:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal utterances processed: {total_utterances}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TARS GPU)",
   "language": "python",
   "name": "tars_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
