{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84acb83f",
   "metadata": {},
   "source": [
    "## 3 Whisper Finetuning\n",
    "\n",
    "Now the fun part finally starts. This notebook will finetune the whisper model on romansh to improve transcription performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a948b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict, Audio, Dataset\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32bf744",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3873b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"romansh-data/sursilvan-small/\"\n",
    "output_dir = \"./whisper-medium-rm-finetuned\"\n",
    "\n",
    "model_name = \"openai/whisper-medium\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f0a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining datasets...\n",
      "\n",
      "‚úÖ DatasetDict created:\n",
      "  Train: 1504 samples\n",
      "  Test:  94 samples\n",
      "\n",
      "üìù First 2 training samples:\n",
      "  1. Il davos temps vegn bia discutau dalla rolla dalla dunna ella baselgia catolica, co vesis vus quella...\n",
      "  2. Suenter ina prelecziun facultativa en dretg da bancas el studi ha quei plaschiu fetg bein a mi. Quei...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load and Combine Dataset\n",
    "print(\"Loading and combining datasets...\")\n",
    "\n",
    "# Load all three TSV files\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"train.tsv\"), sep=\"\\t\")\n",
    "validated_df = pd.read_csv(os.path.join(data_path, \"validated.tsv\"), sep=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(data_path, \"test.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Combine train and validated for training\n",
    "train_combined_df = pd.concat([train_df, validated_df], ignore_index=True)\n",
    "\n",
    "# Add full audio paths\n",
    "train_combined_df[\"audio\"] = os.path.join(data_path, \"clips/\") + train_combined_df[\"path\"]\n",
    "test_df[\"audio\"] = os.path.join(data_path, \"clips/\") + test_df[\"path\"]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_combined_df[[\"audio\", \"sentence\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"audio\", \"sentence\"]])\n",
    "\n",
    "# Create DatasetDict\n",
    "common_voice = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(\"\\n‚úÖ DatasetDict created:\")\n",
    "print(f\"  Train: {len(common_voice['train'])} samples\")\n",
    "print(f\"  Test:  {len(common_voice['test'])} samples\")\n",
    "\n",
    "# Optional: Show first few samples\n",
    "print(\"\\nüìù First 2 training samples:\")\n",
    "for i in range(min(2, len(train_combined_df))):\n",
    "    print(f\"  {i+1}. {train_combined_df['sentence'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5858a71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Components loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Whisper components...\")\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_name, task=task)\n",
    "\n",
    "print(\"‚úÖ Components loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5367befe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed19c60ee7c432caac71b960d0171ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset (num_proc=1):   0%|          | 0/1504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c34bff7e2e7412c9d71e4f1e5f98406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset (num_proc=1):   0%|          | 0/94 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset prepared\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Alternative - More explicit audio loading\n",
    "print(\"Preparing dataset...\")\n",
    "\n",
    "def prepare_dataset_manual(batch):\n",
    "    \"\"\"Manually load and process each audio file\"\"\"\n",
    "    processed_features = []\n",
    "    processed_labels = []\n",
    "    \n",
    "    for i in range(len(batch[\"audio\"])):\n",
    "        audio_path = batch[\"audio\"][i]\n",
    "        \n",
    "        # Load audio with librosa (more control)\n",
    "        audio_array, sampling_rate = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        # Compute features\n",
    "        input_features = feature_extractor(\n",
    "            audio_array, \n",
    "            sampling_rate=16000\n",
    "        ).input_features[0]\n",
    "        processed_features.append(input_features)\n",
    "        \n",
    "        # Encode text\n",
    "        labels = tokenizer(batch[\"sentence\"][i]).input_ids\n",
    "        processed_labels.append(labels)\n",
    "    \n",
    "    batch[\"input_features\"] = processed_features\n",
    "    batch[\"labels\"] = processed_labels\n",
    "    return batch\n",
    "\n",
    "# Apply the manual preparation\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset_manual,\n",
    "    batched=True,\n",
    "    batch_size=16,  # Process 16 at a time\n",
    "    remove_columns=common_voice.column_names[\"train\"],\n",
    "    num_proc=1,\n",
    "    desc=\"Preparing dataset\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95939637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Data Collator\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06392e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluation Metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917337bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model: openai/whisper-medium...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b8e9d052dc4d568a4bdd92f78a76c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded (763.9M parameters)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading Whisper model: {model_name}...\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Disable cache during training\n",
    "model.config.use_cache = False\n",
    "\n",
    "# For Romansh, set forced decoder ids for the task\n",
    "# IMPORTANT: Do NOT modify model.config.suppress_tokens directly!\n",
    "if hasattr(processor, \"get_decoder_prompt_ids\"):\n",
    "    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(task=task)\n",
    "\n",
    "# Remove this line completely:\n",
    "# model.config.suppress_tokens = []   ‚Üê DELETE THIS LINE\n",
    "\n",
    "print(f\"‚úÖ Model loaded ({sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f825896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments:\n",
      "  Batch size: 16\n",
      "  Learning rate: 1e-05\n",
      "  Max steps: 4000\n",
      "  FP16: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Training Arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments:\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "503f074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896a88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 122/4000 07:09 < 3:51:17, 0.28 it/s, Epoch 1.29/43]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 12: Train!\n",
    "print(\"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Save Model\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TARS GPU)",
   "language": "python",
   "name": "tars_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
